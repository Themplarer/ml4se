{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Упражнение\n",
    "\n",
    "`На основе существующих открытых обученных моделей (CodeBERT, InCoder) собрать решение для суммаризации кода.`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Код был взят из [репозитория](https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/code2nl) и адаптирован"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://code-summary.s3.amazonaws.com/pytorch_model.bin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Модель и вспомогательный код"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self, size, sos, eos, device):\n",
    "        self.size = size\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            self.tt = torch.cuda\n",
    "        elif device == \"cpu\":\n",
    "            self.tt = torch\n",
    "\n",
    "        self.scores = self.tt.FloatTensor(size).zero_() # The score for each translation on the beam.\n",
    "        self.prevKs = [] # The backpointers at each time-step.\n",
    "        self.nextYs = [self.tt.LongTensor(size).fill_(0)]  # The outputs at each time-step.\n",
    "        self.nextYs[0][0] = sos\n",
    "        self._eos = eos # Has EOS topped the beam yet.\n",
    "        self.eosTop = False\n",
    "        self.finished = [] # Time and k pair for finished.\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"\"\"Get the outputs for the current timestep.\"\"\"\n",
    "        return self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"\"\"Get the backpointers for the current timestep.\"\"\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = bestScoresId // numWords\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                self.finished.append((self.scores[i], len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >= self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished = []\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i))\n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished += unfinished[: self.size - len(self.finished)]\n",
    "        return self.finished[: self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps = []\n",
    "        for _, timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j + 1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "\n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence = []\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok == self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Build Seqence-to-Sequence.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * `encoder`- encoder of seq2seq model. e.g. roberta\n",
    "    * `decoder`- decoder of seq2seq model. e.g. transformer\n",
    "    * `config`- configuration of encoder model.\n",
    "    * `beam_size`- beam size for beam search.\n",
    "    * `max_length`- max length of target for beam search.\n",
    "    * `sos_id`- start of symbol ids in target for beam search.\n",
    "    * `eos_id`- end of symbol ids in target for beam search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, config, beam_size=None,\n",
    "        max_length=None, sos_id=None, eos_id=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.config = config\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "        self.tie_weights()\n",
    "\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "\n",
    "    def _tie_or_clone_weights(self, first_module, second_module):\n",
    "        \"\"\"Tie or clone module weights depending of weither we are using TorchScript or not\"\"\"\n",
    "        if self.config.torchscript:\n",
    "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
    "        else:\n",
    "            first_module.weight = second_module.weight\n",
    "\n",
    "    def tie_weights(self):\n",
    "        \"\"\"Make sure we are sharing the input and output embeddings.\n",
    "        Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
    "        \"\"\"\n",
    "        self._tie_or_clone_weights(\n",
    "            self.lm_head, self.encoder.embeddings.word_embeddings\n",
    "        )\n",
    "\n",
    "    def forward(self, source_ids, source_mask):\n",
    "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
    "        encoder_output = outputs[0].permute([1, 0, 2]).contiguous()\n",
    "        preds = []\n",
    "\n",
    "        if source_ids.device.type == \"cuda\":\n",
    "            zero = torch.cuda.LongTensor(1).fill_(0)\n",
    "        elif source_ids.device.type == \"cpu\":\n",
    "            zero = torch.LongTensor(1).fill_(0)\n",
    "\n",
    "        for i in range(source_ids.shape[0]):\n",
    "            beam = Beam(\n",
    "                self.beam_size,\n",
    "                self.sos_id,\n",
    "                self.eos_id,\n",
    "                device=source_ids.device.type,\n",
    "            )\n",
    "            context = encoder_output[:, i:i + 1].repeat(1, self.beam_size, 1)\n",
    "            context_mask = source_mask[i:i + 1, :].repeat(self.beam_size, 1)\n",
    "            input_ids = beam.getCurrentState()\n",
    "\n",
    "            for _ in range(self.max_length):\n",
    "                if beam.done():\n",
    "                    break\n",
    "\n",
    "                attn_mask = -1e4 * (\n",
    "                    1 - self.bias[:input_ids.shape[1], :input_ids.shape[1]]\n",
    "                )\n",
    "                tgt_embeddings = (\n",
    "                    self.encoder.embeddings(input_ids)\n",
    "                    .permute([1, 0, 2])\n",
    "                    .contiguous()\n",
    "                )\n",
    "                out = self.decoder(\n",
    "                    tgt_embeddings,\n",
    "                    context,\n",
    "                    tgt_mask=attn_mask,\n",
    "                    memory_key_padding_mask=(1 - context_mask).bool(),\n",
    "                )\n",
    "                out = torch.tanh(self.dense(out))\n",
    "                hidden_states = out.permute([1, 0, 2]).contiguous()[:, -1, :]\n",
    "                out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                beam.advance(out)\n",
    "                input_ids.data.copy_(\n",
    "                    input_ids.data.index_select(0, beam.getCurrentOrigin())\n",
    "                )\n",
    "                input_ids = torch.cat((input_ids, beam.getCurrentState()), -1)\n",
    "\n",
    "            hyp = beam.getHyp(beam.getFinal())\n",
    "            pred = beam.buildTargetTokens(hyp)[: self.beam_size]\n",
    "            pred = [\n",
    "                torch.cat(\n",
    "                    [x.view(-1) for x in p] + [zero] * (self.max_length - len(p))\n",
    "                ).view(1, -1)\n",
    "                for p in pred\n",
    "            ]\n",
    "            preds.append(torch.cat(pred, 0).unsqueeze(0))\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        return preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class InputFeatures:\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "\n",
    "    def __init__(self, example_id, source_ids,\n",
    "        target_ids, source_mask, target_mask):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer):\n",
    "    features = []\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        # source\n",
    "        source_tokens = tokenizer.tokenize(example)[: 256 - 2]\n",
    "        source_tokens = [tokenizer.cls_token] + source_tokens + [tokenizer.sep_token]\n",
    "        source_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        padding_length = 256 - len(source_ids)\n",
    "        source_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        source_mask += [0] * padding_length\n",
    "\n",
    "        target_tokens = tokenizer.tokenize(\"None\")\n",
    "        target_tokens = [tokenizer.cls_token] + target_tokens + [tokenizer.sep_token]\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] * len(target_ids)\n",
    "        padding_length = 128 - len(target_ids)\n",
    "        target_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        target_mask += [0] * padding_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_index,\n",
    "                source_ids,\n",
    "                target_ids,\n",
    "                source_mask,\n",
    "                target_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## We are defining all the needed functions here.\n",
    "def inference(data, model, tokenizer):\n",
    "    # Calculate bleu\n",
    "    eval_sampler = SequentialSampler(data)\n",
    "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
    "\n",
    "    model.eval()\n",
    "    p = []\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to('cpu') for t in batch)\n",
    "        source_ids, source_mask = batch\n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids=source_ids, source_mask=source_mask)\n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[: t.index(0)]\n",
    "                text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
    "                p.append(text)\n",
    "    return p, source_ids.shape[-1]\n",
    "\n",
    "\n",
    "def get_features(examples, tokenizer):\n",
    "    features = convert_examples_to_features(examples, tokenizer)\n",
    "    all_source_ids = torch.tensor(\n",
    "        [f.source_ids[: 256] for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_source_mask = torch.tensor(\n",
    "        [f.source_mask[: 256] for f in features], dtype=torch.long\n",
    "    )\n",
    "    return TensorDataset(all_source_ids, all_source_mask)\n",
    "\n",
    "\n",
    "def build_model(model_class, config, tokenizer):\n",
    "    encoder = model_class(config=config)\n",
    "    decoder_layer = nn.TransformerDecoderLayer(\n",
    "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
    "    )\n",
    "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "    model = Seq2Seq(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        config=config,\n",
    "        beam_size=10,\n",
    "        max_length=128,\n",
    "        sos_id=tokenizer.cls_token_id,\n",
    "        eos_id=tokenizer.sep_token_id,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            \"pytorch_model.bin\",\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        ),\n",
    "        strict=False,\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"microsoft/codebert-base\"\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "\n",
    "model = build_model(model_class=RobertaModel, config=config, tokenizer=tokenizer).to('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Вычисление"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "    return inference(get_features([example], tokenizer), model, tokenizer)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Примеры"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(evaluate(\"\"\"\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def f(a, b):\n",
    "    return a + b\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def smart_algo(*args):\n",
    "    l = len(args)\n",
    "    args = list(sorted(args))\n",
    "\n",
    "    if l % 2 == 1:\n",
    "        return args[l // 2]\n",
    "\n",
    "    return (args[l // 2 - 1] + args[l // 2]) / 2\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def get_median(*args):\n",
    "    l = len(args)\n",
    "    args = list(sorted(args))\n",
    "\n",
    "    if l % 2 == 1:\n",
    "        return args[l // 2]\n",
    "\n",
    "    return (args[l // 2 - 1] + args[l // 2]) / 2\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def f():\n",
    "    '''\n",
    "    Does completely nothing\n",
    "    '''\n",
    "    pass\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def BFS(graph, start):\n",
    "    visited = [False] * len(graph)\n",
    "    distances = [-1] * len(graph)\n",
    "    queue = []\n",
    "\n",
    "    queue.append(start)\n",
    "    visited[start] = True\n",
    "    distances[start] = 0\n",
    "\n",
    "    while queue:\n",
    "        n = queue.pop(0)\n",
    "\n",
    "        for i in graph[n]:\n",
    "            if not visited[i]:\n",
    "                queue.append(i)\n",
    "                visited[i] = True\n",
    "                distances[i] = distances[n] + 1\n",
    "\n",
    "    return distances\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def DFS(graph, node, visited=None):\n",
    "    if not visited:\n",
    "        visited = set()\n",
    "        visited.add(node)\n",
    "\n",
    "    yield node\n",
    "\n",
    "    for i in graph[node]:\n",
    "        if i not in visited:\n",
    "            visited.add(i)\n",
    "            DFS(graph, i, visited)\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def DFS(graph, node):\n",
    "    visited = {node}\n",
    "    stack = [node]\n",
    "\n",
    "    yield node\n",
    "\n",
    "    while stack:\n",
    "        n = stack[-1]\n",
    "\n",
    "        for i in graph[n]:\n",
    "            if i not in visited:\n",
    "                visited.add(i)\n",
    "                yield i\n",
    "\"\"\"))\n",
    "\n",
    "print(evaluate(\"\"\"\n",
    "def algo(graph, node):\n",
    "    visited = {node}\n",
    "    stack = [node]\n",
    "\n",
    "    yield node\n",
    "\n",
    "    while stack:\n",
    "        n = stack[-1]\n",
    "\n",
    "        for i in graph[n]:\n",
    "            if i not in visited:\n",
    "                visited.add(i)\n",
    "                yield i\n",
    "\"\"\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "['Add two numbers .']\n",
    "['Add two arrays .']\n",
    "['Greater common multiplication .']\n",
    "['Calculate the objective function .']\n",
    "['Calculate the median value of an iterable .']\n",
    "['This function will be called on every time .']\n",
    "['BFS traversal .']\n",
    "['DFS DFS iterator .']\n",
    "['DFS DFS iterator .']\n",
    "['Iterate over the graph .']\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
